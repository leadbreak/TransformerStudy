{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85ab822-23be-400b-b1f0-4d08a6b9d2b4",
   "metadata": {},
   "source": [
    "# 00.Introduction\n",
    "- ViT에 대해 Library나 Sample로 구현된 것들은 있으나, 하나하나 설명된 것은 없어 이해하기 위해 각 구성요소들을 직접 구현해본다.  \n",
    "    Though there are library or sample code for ViT, there isn't any source to study and understand that. So I would make it for myself or others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640cc9c-23a4-4bda-8980-c810c79db4cc",
   "metadata": {},
   "source": [
    "- 기본적으로 ViT는 Bert와 같이 Transformer의 Encoder만을 가져와 사용하는 방식이며, 이는 데이터를 추상화하고 이에 대한 최종 해석값을 내놓는 과정으로 이해할 수 있다.  \n",
    "  네 발로 걷는 것, 나는 것, 날개가 달린 것, 털이 있는 것이나 비늘이 있는 것으로 모든 동물을 분류하고, 각 객체들을 판단할 때 어떤 요소에 집중했는지를 파악할 수 있어 일종의 XAI와 같은 역할을 하기도 한다.\n",
    "- ViT는 크게 세 부분으로 구성돼 있으며, 각각 Input Layer / Encoder / MLP Head 이다.  \n",
    "  이 중 Encoder는 여러 개의 Encoder Block으로 구성돼 있으며, 각각의 Encoder Block은 Self-Attention과 MLP로 구성돼 있다.\n",
    "  Self-Attention은 일반적으로 Multi-head Attention 방식으로 구현되며, MLP는 다른 사전학습 레이어(ex. Resnet)로 대체될 수 있으나,  \n",
    "  복수의 Encoder Block이 들어가는 과정에서 대량의 메모리와 처리속도를 고려하면 간단한 레이어를 사용하는 것이 현재로선 좋아보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa99f7-3be5-4a14-9386-6a7cfddddf8c",
   "metadata": {},
   "source": [
    "# 01.Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d40eb5-45c3-4ed4-99c6-3f50d747a778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8]) tensor([[[-1.4656, -0.0863, -1.3829,  0.7328, -0.2383,  1.3856,  0.3638,\n",
      "          -0.4458]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn(1,1,8)\n",
    "print(a.shape, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490655b3-ff82-47d2-bcc4-9e425a5eb8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VitInputLayer(nn.Module) :\n",
    "    def __init__(self,\n",
    "                 in_channels:int=3,    # 입력 채널 수\n",
    "                 emb_dim:int=384,      # embedding vector의 길이\n",
    "                 num_patch_row:int=2,  # 분할할 Patch 단위(Height를 기준으로 - 보통 정사각형으로 처리)\n",
    "                 image_size:int=32     # 입력 이미지 한 변의 길이\n",
    "                ) :\n",
    "        super(VitInputLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_patch_row = num_patch_row\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # STEP 01. patch 수대로 입력 이미지 분할\n",
    "        ## 기본 입력값(num_patch_row)대로 분할한다면, 2x2=4개의 이미지로 분할됨\n",
    "        self.num_patch = self.num_patch_row**2\n",
    "        \n",
    "        ## patch에 따른 size를 계산하고, 만약 사이즈가 떨어지지 않으면 error\n",
    "        self.patch_size = int(self.image_size / self.num_patch_row)\n",
    "        assert self.image_size % self.num_patch_row == 0, \"patch size doesn't match with image size\"\n",
    "        \n",
    "        ## 입력 이미지를 Patch로 분할하고, Patch 단위로 Embedding하기 위한 레이어 구축\n",
    "        self.patch_emb_layer = nn.Conv2d(\n",
    "                                        in_channels=self.in_channels,\n",
    "                                        out_channels=self.emb_dim,\n",
    "                                        kernel_size=self.patch_size,\n",
    "                                        stride=self.patch_size\n",
    "                                        )\n",
    "        \n",
    "        # STEP 02. cls token & position embedding\n",
    "        ## class token(cls token)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_dim)) # (1, 1, emb_dim) 차원의 Parameter(변경가능한 값)을 정의\n",
    "        \n",
    "        ## pos embedding for sequential info(This is general function in NLP, but optional in CV)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, self.num_patch+1, emb_dim))\n",
    "        \n",
    "        \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor : \n",
    "        \"\"\"\n",
    "        x : (B:batch_size, C:channel_nums, H:height, W:width) 차원의 input image\n",
    "        \n",
    "        z_0 : (B:batch_size, N:token_nums, D:dim_of_embedding_vector) 차원의 ViT 입력\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # STEP 03. Patch Embedding & Flatten\n",
    "        ## Patch Embedding : (B, C, H, W) -> (B, D, H/P, W/P)\n",
    "        z_0 = self.patch_emb_layer(x)\n",
    "        \n",
    "        ## Flatten : (B, D, H/P, W/P) -> (B, D, Np)\n",
    "        ### Np : patch_nums = ((H*W)/(P^2))\n",
    "        z_0 = z_0.flatten(2)\n",
    "        \n",
    "        ## Transpose : (B, D, Np) -> (B, Np, D)\n",
    "        z_0 = z_0.transpose(1,2)\n",
    "        \n",
    "        # STEP 04. Patch Embedding + cls token\n",
    "        ## (B, Np, D) -> (B, N, D)\n",
    "        ### N = (Np+1)\n",
    "        z_0 = torch.cat([self.cls_token.repeat(repeats=(x.size(0),1,1)), z_0], dim=1)\n",
    "        \n",
    "        ## + pos embedding\n",
    "        z_0 = z_0 + self.pos_emb\n",
    "        \n",
    "        return z_0 # (B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048b19af-5e40-4177-aea2-f2fd118c53df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n",
      "tensor([[[ 0.8181,  1.8900, -0.0716,  ...,  1.2480,  1.6079,  2.0446],\n",
      "         [ 0.7724, -1.4483,  0.2489,  ..., -2.6664, -0.4604,  0.9533],\n",
      "         [-0.9500, -2.0801, -1.5512,  ...,  0.3810, -0.2633,  1.0653],\n",
      "         [-0.4632,  0.8257, -1.2454,  ...,  2.3331, -0.8414, -0.5579],\n",
      "         [-1.2910,  1.3327,  0.4927,  ...,  0.2185,  0.2061,  0.5208]],\n",
      "\n",
      "        [[ 0.8181,  1.8900, -0.0716,  ...,  1.2480,  1.6079,  2.0446],\n",
      "         [ 1.1584, -1.3264, -1.2515,  ..., -3.3588, -0.2809,  1.4500],\n",
      "         [-1.5001, -1.0683, -1.7576,  ...,  1.7348,  0.2732,  1.6792],\n",
      "         [-1.0329, -0.0263, -0.3200,  ...,  0.7118, -0.8149, -0.2893],\n",
      "         [-1.2318,  0.8022, -0.8656,  ...,  0.5152,  0.8738,  0.1135]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Input Layer Check\n",
    "\n",
    "import torch\n",
    "\n",
    "batch_size, channel, height, width = 2,3,32,32\n",
    "x = torch.randn(batch_size, channel, height, width)\n",
    "input_layer = VitInputLayer(num_patch_row=2)\n",
    "z_0 = input_layer(x)\n",
    "\n",
    "print(z_0.shape) # (B, N, D)\n",
    "print(z_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbce886-af4b-4052-890e-eda8135c671d",
   "metadata": {},
   "source": [
    "# 02. Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a4dc70-31ff-4d96-b39e-2a12bd016687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module) :\n",
    "    def __init__(self,\n",
    "                 emb_dim:int=384,  # embedding vector 길이\n",
    "                 head:int=3,       # head 개수\n",
    "                 dropout:float=0.1  # dropout rate\n",
    "                ) :\n",
    "        \n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.head = head\n",
    "        self.head_dim = emb_dim // head\n",
    "        self.sqrt_dh = self.head_dim**0.5 # scaling factor로 나눔으로써 feature dimension 구현\n",
    "        \n",
    "        # STEP 01. Define Layers\n",
    "        ## Linear Layer for Query, Key, Value weights\n",
    "        self.w_q = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.w_k = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.w_v = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        \n",
    "        ## Dropout Layer\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # MSHA's output layer\n",
    "        self.w_o = nn.Sequential(\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z:torch.Tensor) -> torch.tensor :\n",
    "        \"\"\"\n",
    "        z : (B:batch_size, N:token_nums, D:vector_dims) 차원 MHSA 입력\n",
    "        \n",
    "        out : (B:batch_size, N:token_nums, D:embedding_vector_dims) 차원 MHSA 출력\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, num_patch, _ = z.size()\n",
    "        \n",
    "        # STEP 02. calculate self attention score \n",
    "        ## q, k, v embedding\n",
    "        ## (B, N, D) -> (B, N, D)\n",
    "        q = self.w_q(z)\n",
    "        k = self.w_k(z)\n",
    "        v = self.w_v(z)\n",
    "        \n",
    "        ## Attention Score 계산을 위한 사전작업\n",
    "        ## (B, N, D) -> (B, N, h, D//h)\n",
    "        q = q.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "        k = k.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "        v = v.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "        \n",
    "        ## (B, N, h, D//h) -> (B, h, N, D//h)\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        ## 내적 : matmul\n",
    "        ## k_T : (B, h, N, D//h) -> (B, h, D/h, N)\n",
    "        k_T = k.transpose(2,3)\n",
    "        \n",
    "        ## QKt : (B,h,N,D//h)@(B,h,D//h,N)=(B,h,N,N)\n",
    "        dots = (q@k_T) / self.sqrt_dh\n",
    "        \n",
    "        ## 열방향 softmax 값 & dropout\n",
    "        attn = F.softmax(dots, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        # 가중화\n",
    "        ## (B,h,N,N)@(B,h,N,D//h)=(B,h,N,D//h)\n",
    "        out = attn @ v\n",
    "        \n",
    "        ## (B, h, N, D//h) -> (B, N, h, D//h)\n",
    "        out = out.transpose(1,2)\n",
    "        \n",
    "        ## (B, N, h, D//h) -> (B, N, D)\n",
    "        out = out.reshape(batch_size, num_patch, self.emb_dim)\n",
    "        \n",
    "        ## 출력층 : (B, N, D) -> (B, N, D)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "273a43d6-72db-478a-870d-784c43be218a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n",
      "tensor([[[ 0.0944, -0.0101,  0.0938,  ...,  0.0000,  0.1515,  0.2264],\n",
      "         [ 0.0476,  0.0749, -0.1221,  ...,  0.1416,  0.0000,  0.0889],\n",
      "         [ 0.0370, -0.1745,  0.0087,  ...,  0.4039,  0.1903,  0.1085],\n",
      "         [-0.0458, -0.0173,  0.0980,  ...,  0.0000,  0.2504, -0.0000],\n",
      "         [ 0.0447, -0.0513, -0.0082,  ...,  0.4133,  0.1874,  0.0523]],\n",
      "\n",
      "        [[ 0.0236,  0.2666, -0.0194,  ...,  0.2342,  0.1559,  0.0163],\n",
      "         [-0.0915,  0.0011,  0.2990,  ...,  0.4258,  0.1204,  0.0000],\n",
      "         [ 0.0296, -0.0000,  0.0014,  ...,  0.5210, -0.0426,  0.1725],\n",
      "         [ 0.0365,  0.1460,  0.0578,  ...,  0.0000,  0.1876, -0.0705],\n",
      "         [ 0.0720,  0.1341,  0.1422,  ...,  0.3279, -0.0400,  0.1927]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# MHSA Test\n",
    "\n",
    "mhsa = MultiHeadSelfAttention()\n",
    "out = mhsa(z_0)\n",
    "\n",
    "# (B=2,N=5,D=384)\n",
    "print(out.shape, out, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2c75e-40bc-40f4-8342-5339516b0700",
   "metadata": {},
   "source": [
    "# 03. Encoder\n",
    "- 일반적으로 Encoder는 복수의 Encoder Block으로 구성되며, 각 Encoder Block은 **[LayerNorm->MHSA->LayerNorm->MLP]** 로 구성되어 있습니다.  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4272ea71-dc22-4376-ae76-8eb315b48afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VitEncoderBlock(nn.Module) :\n",
    "    def __init__(\n",
    "                self,\n",
    "                emb_dim:int=384,\n",
    "                head:int=8,\n",
    "                hidden_dim:int=384*4,\n",
    "                dropout:float=0.1\n",
    "                ) :\n",
    "        \n",
    "        super(VitEncoderBlock, self).__init__()\n",
    "        \n",
    "        # STEP 01. Define Encoder Block Layers\n",
    "        ## 1st Layer Norm\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        ## MHSA\n",
    "        self.mhsa = MultiHeadSelfAttention(\n",
    "                                           emb_dim = emb_dim,\n",
    "                                           head=head,\n",
    "                                           dropout=dropout,\n",
    "                                           )\n",
    "        \n",
    "        ## 2nd Layer Norm\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        ## MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "                                nn.Linear(emb_dim, hidden_dim),\n",
    "                                nn.GELU(),\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.Linear(hidden_dim, emb_dim),\n",
    "                                nn.Dropout(dropout),\n",
    "                                )\n",
    "        \n",
    "    def forward(self, z:torch.Tensor) -> torch.Tensor :\n",
    "        \"\"\"\n",
    "        z : (B:batch_size, N:token_nums, D:vector_dims) 차원 Encoder Block 입력\n",
    "        \n",
    "        out : (B:batch_size, N:token_nums, D:embedding_vector_dims) 차원 Encoder Block 출력\n",
    "        \"\"\"\n",
    "        \n",
    "        # STEP 02. Construct Encoder Block\n",
    "        ## 하나의 Encoder Block은 Layer Norm을 기준으로 크게 둘로 나뉘어져 있으며,\n",
    "        ## 이 과정에서 Residual connection이 고려된다\n",
    "        \n",
    "        ### part 1 : MHSA(layerNorm)+ResidualConnection1\n",
    "        out = self.mhsa(self.ln1(z)) + z\n",
    "        \n",
    "        ### part 2 : MLP(layerNorm)+ResidualConnection2\n",
    "        out = self.mlp(self.ln2(z)) + out\n",
    "        \n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5618f2af-18df-4e35-9a33-7dc91e960473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n",
      "tensor([[[ 0.2280,  2.2770,  0.0095,  ...,  1.6610,  1.1083,  3.0569],\n",
      "         [ 0.2297, -1.3305,  0.4470,  ..., -2.5948, -1.0139,  1.5729],\n",
      "         [-1.5734, -1.7607, -1.6213,  ...,  0.2934, -0.5840,  1.1330],\n",
      "         [-1.1030,  0.4799, -1.5101,  ...,  2.5298, -0.8372, -0.9231],\n",
      "         [-1.6123,  1.7179,  0.4211,  ...,  0.1317, -0.3342,  0.8397]],\n",
      "\n",
      "        [[ 0.2411,  1.9997, -0.0132,  ...,  1.3102,  1.3436,  3.0914],\n",
      "         [ 1.0210, -1.3893, -0.8584,  ..., -3.6101, -0.7249,  2.0603],\n",
      "         [-2.2505, -1.1484, -1.9839,  ...,  1.3386, -0.0072,  2.0938],\n",
      "         [-1.3323, -0.0995, -0.7837,  ...,  0.3104, -0.4181, -0.6488],\n",
      "         [-1.5087,  1.0819, -0.3002,  ...,  0.3063,  0.7543,  0.6095]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Encoder Block Test\n",
    "\n",
    "vit_enc = VitEncoderBlock()\n",
    "z_1 = vit_enc(z_0)\n",
    "\n",
    "# (B=2, N=5, D=384)\n",
    "print(z_1.shape, z_1, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ddd154-9350-447f-b89b-737b53667049",
   "metadata": {},
   "source": [
    "# 04. Vision Transformer\n",
    "- 이번에는 이전 단계에서 구축한 input layer/Encoder에 MLP를 더해 전체 ViT를 구현해보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0507b590-e144-44ae-a4ad-cd961bf53878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ViT(nn.Module) :\n",
    "    def __init__(self,\n",
    "                 in_channels:int=3,\n",
    "                 num_classes:int=10,\n",
    "                 emb_dim:int=384,\n",
    "                 num_patch_row:int=2,\n",
    "                 image_size:int=32,\n",
    "                 num_blocks:int=7,     # Encoder Block의 수\n",
    "                 head:int=8,\n",
    "                 hidden_dim:int=384*4,\n",
    "                 dropout:float=0.1,\n",
    "                ) :\n",
    "        \n",
    "        super(ViT, self).__init__()\n",
    "        \n",
    "        # STEP 01. Input Layer\n",
    "        self.input_layer = VitInputLayer(in_channels,\n",
    "                                         emb_dim,\n",
    "                                         num_patch_row,\n",
    "                                         image_size)\n",
    "        \n",
    "        # STEP 02. Encoder = Encoder Block x num_blocks\n",
    "        self.encoder = nn.Sequential(*[VitEncoderBlock(emb_dim,\n",
    "                                                       head,\n",
    "                                                       hidden_dim,\n",
    "                                                       dropout\n",
    "                                                      )\n",
    "                                       for _ in range(num_blocks)])\n",
    "        \n",
    "        \n",
    "        # STEP 03. MLP Head\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(emb_dim),\n",
    "                                      nn.Linear(emb_dim, num_classes)\n",
    "                                     )\n",
    "        \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor :\n",
    "        \"\"\"\n",
    "        x : (B:batch_size, C:channel_nums, H:height, W:width) 차원의 ViT 입력 이미지 \n",
    "        \n",
    "        out : (B:batch_size, M:class_nums) 차원의 ViT 출력값\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # STEP 04. Construct ViT\n",
    "        \n",
    "        ## Input Layer : (B,C,H,W)->(B,N,D)\n",
    "        ## N : num_tokens, D : dim_vector\n",
    "        out = self.input_layer(x)\n",
    "        \n",
    "        ## Encoder : (B,N,D)->(B,N,D)\n",
    "        out = self.encoder(out)\n",
    "        \n",
    "        ## class token : (B,N,D)->(B,D)\n",
    "        cls_token = out[:,0]\n",
    "        \n",
    "        ## MLP Head : (B,D)->(B,M)\n",
    "        pred = self.mlp_head(cls_token)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90313de7-0dab-4a10-9402-5714bbedc13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n",
      "tensor([[-0.4547,  0.4218, -0.1759, -0.9137,  0.2284, -0.4648,  0.0166, -0.8592,\n",
      "         -0.4822, -0.8670],\n",
      "        [-0.6414,  0.1693, -0.1344, -0.8408,  0.2250, -0.3415,  0.3283, -1.1152,\n",
      "         -0.4313, -0.7437]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_classes = 10\n",
    "batch_size, channel, height, width = 2, 3, 32, 32\n",
    "x = torch.randn(batch_size, channel, height, width)\n",
    "vit = ViT(in_channels=channel, num_classes=num_classes)\n",
    "pred = vit(x)\n",
    "\n",
    "# (B=2, M=10)\n",
    "print(pred.shape, pred, sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
